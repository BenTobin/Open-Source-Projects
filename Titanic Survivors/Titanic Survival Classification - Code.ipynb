{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic survival prediction\n",
    "\n",
    "\n",
    "\n",
    "In this part we will attempt to predict who survived on the Titanic. The data set we use has the following vairbales (features/attributes):\n",
    "\n",
    "\n",
    "|  Variable   |          Definition          |              Key/Values                 |\n",
    "|:-----------:|:----------------------------:|:---------------------------------------:|\n",
    "| PassengerId | Index                        |integer                                  |\n",
    "| Pclass      | Ticket class                 |1=1st, 2=2nd, 3=3rd                      |\n",
    "| Name        | Name of passenger            |string                                   |\n",
    "| Sex         | Sex                          |male, female                             |\n",
    "| Age         | Age in years                 |integer                                  |\n",
    "| SibSp       | # of siblings/spouses aboard |integer                                  |\n",
    "| Parch       | # of parents/children aboard |integer                                  |\n",
    "| Ticket      | Ticket number                |string                                   |\n",
    "| Fare        | Ticket fare                  |float                                    |\n",
    "| Cabin       | Cabin number                 |a code                                   |\n",
    "| Embarked    | Port of Embarkation          |C=Cherbourg, Q=Queenstown, S=Southampton |\n",
    "| **Survived**| Predicted varibale           |0=No, 1=Yes                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "\n",
    "This part of the project is a competetive one. The goal is to produce the best prediciton you can\n",
    "<br><br>\n",
    "\n",
    "\n",
    "**Methodogology**\n",
    "\n",
    "So far you only know few methods for classification: KNN, logistic regression and SVM. You can use each one of them. You can also use linear regression, but then you need to convert the output to 0s and 1s (this is not a straight forward use of linear regression but a possible one). You may want to choose which features to use (it could be that some features are not useful). Further, some features have missing values. You will need to decide how to handle this (for instance: drop rows with missing values, place an avergae value in those rows, or some other method of your choosing). Another matter to consider is handling non-numeric values. For example, sex is non-numeric. You may choose to drop non-numeric features, or you could convert them to numeric values (if such conversion makes sense).\n",
    "Also you will need to consider splitting the data into a training set and a test set so as to avoid tailoring the solution (overfitting) to the data you have.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "**Model Output**\n",
    "\n",
    "Your model needs to produce a prediciton for each data sample (row), which is 0 (did not survive) or 1 (survived). \n",
    "<br><br>\n",
    "\n",
    "**Scoring your model**\n",
    "\n",
    "Your model performance will be assessed on test data that is not available to you. As mentioned above, this part of the project is a competition, where the goal is to achieve highest model accuracy\n",
    "\n",
    "<br><br>\n",
    "**Final Note**\n",
    "\n",
    "This part of the project is open ended, in that you are not given small and specific tasks. However, you are already familiar with all the components needed to succeed. Specifically, reading data into pandas dataframe, dropping columns, dropping rows, changing value of features, splitting data into train and test subsets, performing model using sklearn library, and using cross validation. So don't panic...\n",
    "\n",
    "\n",
    "GOOD LUCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Libraries, Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Initial Libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer # used for handling missing data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # used for encoding categorical data\n",
    "from sklearn.model_selection import train_test_split # used for splitting training and testing data\n",
    "from sklearn.preprocessing import StandardScaler # used for feature scaling\n",
    "from sklearn.linear_model import LogisticRegression # logistic regression model\n",
    "from sklearn.svm import SVC # svm model\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN model\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold b\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "titanic = pd.read_csv('titanic_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a ) Embarked - One Hot Encoding\n",
    "We changed Embarked in to three binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embarked = pd.get_dummies(titanic.Embarked, prefix='Embarked_')\n",
    "titanic['Embarked_C'] = Embarked.iloc[:,0]\n",
    "titanic['Embarked_Q'] = Embarked.iloc[:,1]\n",
    "titanic['Embarked_S'] = Embarked.iloc[:,2]\n",
    "\n",
    "# Removing rows where embarked are NA:\n",
    "# There were only two so we decided its better to remove them.\n",
    "titanic = titanic[titanic['Embarked'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  b) Family Size - Sib+ Parch\n",
    "Adding a feature represting each passengers family size on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fam_Size = SibSp + Parch\n",
    "fam_size = []\n",
    "for i in range(len(titanic)):\n",
    "    fam_size.append(titanic.iloc[i,5] + titanic.iloc[i,6] + 1)\n",
    "titanic.insert(7,\"Family_Size\",fam_size, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Title - One Hot Encoding\n",
    "A new feature defining the Title of a passenger, we believe this adds information about each passengers Life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by extracting the title from the name feature\n",
    "\n",
    "title = titanic['Name'].str.extract(r\" ([A-Za-z]+)\\. \")\n",
    "titanic['Title'] = title\n",
    "\n",
    "# Minimizing titles\n",
    "## We wanted to mkae specified groups so that we can better evaluate each group (given the amount of data)\n",
    "\n",
    "titanic['Title'] = titanic['Title'].replace(['Lady', 'Capt', 'Col',\n",
    "    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n",
    "titanic['Title'] = titanic['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n",
    "titanic['Title'] = titanic['Title'].replace('Mlle', 'Miss')\n",
    "titanic['Title'] = titanic['Title'].replace('Ms', 'Miss')\n",
    "titanic['Title'] = titanic['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "# Using One Hot Encoding we transformed each title into its own binary feature\n",
    "Title = pd.get_dummies(titanic.Title, prefix='Title_')\n",
    "titanic['Title_Master'] = Title.iloc[:,0]\n",
    "titanic['Title_Miss'] = Title.iloc[:,1]\n",
    "titanic['Title_Mr'] = Title.iloc[:,2]\n",
    "titanic['Title_Mrs'] = Title.iloc[:,3]\n",
    "titanic['Title_Rare'] = Title.iloc[:,4]\n",
    "titanic['Title_Royal'] = Title.iloc[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) Sex- changing this column to a numerical (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_vals = {\"Sex\":     {\"male\": 1, \"female\": 0}}\n",
    "titanic = titanic.replace(cleanup_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e) Age - Addressing NA's in the age feature:\n",
    "We chose to predict the missing ages based on their Gender and Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_NaN_age = list(titanic[\"Age\"][titanic[\"Age\"].isnull()].index)\n",
    "# A list of all the indexs wher age =NaN\n",
    "for i in index_NaN_age:\n",
    "    age_pred = np.nanmedian(titanic[\"Age\"][(titanic['Sex'] == titanic.iloc[i][\"Sex\"]) & \n",
    "                                           (titanic['Pclass'] == titanic.iloc[i][\"Pclass\"])])\n",
    "    titanic.loc[i, 'Age'] = age_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f) Pclass - One Hot Encoding\n",
    "We wanted to change Pclass from 1 catergorial feature to 3 binary ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning \"Pclass\" into three seperate binary columns.\n",
    "Pclass = pd.get_dummies(titanic.Pclass, prefix='Pclass_')\n",
    "titanic['Pclass_1'] = Pclass.iloc[:,0]\n",
    "titanic['Pclass_2'] = Pclass.iloc[:,1]\n",
    "titanic['Pclass_3'] = Pclass.iloc[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we also tried adding polynomial of the continuoes values, but they lowered the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Data Cleaning, Scaling and Splitting into Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reordering Our Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic[['PassengerId',\n",
    " 'Pclass',\n",
    " 'Name',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Family_Size',\n",
    " 'Ticket',\n",
    " 'Fare',\n",
    " 'Cabin',\n",
    " 'Embarked',\n",
    " 'Embarked_C',\n",
    " 'Embarked_Q',\n",
    " 'Embarked_S',\n",
    " 'Title',\n",
    " 'Title_Master',\n",
    " 'Title_Miss',\n",
    " 'Title_Mr',\n",
    " 'Title_Mrs',\n",
    " 'Title_Rare',\n",
    " 'Title_Royal',\n",
    " 'Pclass_1',\n",
    " 'Pclass_2',\n",
    " 'Pclass_3','Survived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Rare</th>\n",
       "      <th>Title_Royal</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352081</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533123</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.051237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.366704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444795</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.986502</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268139</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.312711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.975253</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.119235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.910011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.318612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280757</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.221098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.921260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.646688</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Sex       Age  SibSp  Parch  Family_Size      Fare  \\\n",
       "0     0.352081  1.0  0.533123  0.125    0.2          0.2  0.051237   \n",
       "1     0.366704  0.0  0.444795  0.000    0.0          0.0  0.025374   \n",
       "2     0.986502  1.0  0.305994  0.000    0.0          0.0  0.015412   \n",
       "3     0.321710  1.0  0.268139  0.000    0.0          0.0  0.015412   \n",
       "4     0.312711  0.0  0.432177  0.125    0.2          0.2  0.039525   \n",
       "5     0.975253  1.0  0.305994  0.000    0.0          0.0  0.018543   \n",
       "6     0.119235  1.0  0.305994  0.000    0.0          0.0  0.015176   \n",
       "7     0.910011  1.0  0.318612  0.000    0.0          0.0  0.015395   \n",
       "8     0.440945  0.0  0.280757  0.125    0.0          0.1  0.221098   \n",
       "9     0.921260  0.0  0.646688  0.125    0.2          0.2  0.182500   \n",
       "\n",
       "   Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  \\\n",
       "0         0.0         0.0         1.0           0.0         0.0       1.0   \n",
       "1         0.0         0.0         1.0           0.0         0.0       0.0   \n",
       "2         0.0         0.0         1.0           0.0         0.0       1.0   \n",
       "3         0.0         0.0         1.0           0.0         0.0       1.0   \n",
       "4         0.0         0.0         1.0           0.0         0.0       0.0   \n",
       "5         0.0         0.0         1.0           0.0         0.0       1.0   \n",
       "6         0.0         0.0         1.0           0.0         0.0       1.0   \n",
       "7         0.0         0.0         1.0           0.0         0.0       1.0   \n",
       "8         1.0         0.0         0.0           0.0         1.0       0.0   \n",
       "9         0.0         0.0         1.0           0.0         0.0       0.0   \n",
       "\n",
       "   Title_Mrs  Title_Rare  Title_Royal  Pclass_1  Pclass_2  Pclass_3  Survived  \n",
       "0        0.0         0.0          0.0       0.0       1.0       0.0       0.0  \n",
       "1        1.0         0.0          0.0       0.0       1.0       0.0       1.0  \n",
       "2        0.0         0.0          0.0       0.0       0.0       1.0       0.0  \n",
       "3        0.0         0.0          0.0       0.0       0.0       1.0       0.0  \n",
       "4        1.0         0.0          0.0       0.0       0.0       1.0       1.0  \n",
       "5        0.0         0.0          0.0       0.0       0.0       1.0       0.0  \n",
       "6        0.0         0.0          0.0       0.0       0.0       1.0       1.0  \n",
       "7        0.0         0.0          0.0       0.0       0.0       1.0       0.0  \n",
       "8        0.0         0.0          0.0       1.0       0.0       0.0       1.0  \n",
       "9        1.0         0.0          0.0       1.0       0.0       0.0       1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1:\n",
    "# Removing Features that are non numeric.\n",
    "tbf = titanic.drop(['Name','Ticket','Cabin','Embarked','Pclass', 'Title'] , axis =1)\n",
    "tbf_clean = tbf.dropna()\n",
    "\n",
    "# STEP 2:\n",
    "# Normilizing our Data , We are using Min/Max scaling.\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = tbf_clean.columns\n",
    "d = scaler.fit_transform(tbf_clean)\n",
    "scaled_tbf = pd.DataFrame(d, columns=names)\n",
    "scaled_tbf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting Data to Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_tbf.iloc[:,0:19]\n",
    "y = scaled_tbf.iloc[:,19] \n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
    "# We chose to to split 80/20 to training/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.8380281690140845, 0.7887323943661971, 0.8309859154929577, 0.8309859154929577, 0.8098591549295775]\n",
      "Avg accuracy Logistic Regression: 0.8197183098591548\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# OPTIMAL SVM MODEL #\n",
    "####################################################################################\n",
    "\n",
    "# Implimenting cross validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "LogReg = LogisticRegression(solver= 'lbfgs')\n",
    " \n",
    "acc_score = []\n",
    " \n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "\n",
    "    \n",
    "# Modeling:  \n",
    "    LogReg.fit(X_train,y_train)\n",
    "    pred_values = LogReg.predict(X_test)\n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy Logistic Regression: {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(KNeighborsClassifier(n_neighbors=19), 0.823943661971831)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################\n",
    "# OPTIMAL KNN MODEL #\n",
    "####################################################################################\n",
    "\n",
    "param = [\n",
    "    {'n_neighbors': range(2, 20, 1)}\n",
    "]\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# for this model were using the GridSearch function to optimize model parameters. \n",
    "gs_knn = GridSearchCV(knn, param, cv = 3 , n_jobs = -1)\n",
    "gs_knn.fit(X_train, y_train)\n",
    "\n",
    "knn_best = gs_knn.best_estimator_\n",
    "gs_knn.best_estimator_, gs_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SVC(C=3, gamma=0.3, probability=True), 0.8661971830985915)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################\n",
    "# OPTIMAL SVM MODEL #\n",
    "####################################################################################\n",
    "\n",
    "param = [\n",
    "    {\n",
    "        'kernel': ['rbf'], 'C': [0.1, 0.3, 1, 2, 3, 4], \n",
    "        'gamma': [0.3, 1, 3, 10, 12, 15, 25, 28]\n",
    "    }, \n",
    "]\n",
    "\n",
    "svc = SVC(probability = True)\n",
    "\n",
    "# for this model were using the GridSearch function to optimize model parameters.\n",
    "gs_svc = GridSearchCV(svc, param, cv = 5, n_jobs = -1, verbose = 1)\n",
    "gs_svc.fit(X_train, y_train)\n",
    "svc_best = gs_svc.best_estimator_\n",
    "gs_svc.best_estimator_, gs_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparrison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic:\n",
      "________________________________________\n",
      "L.G Score:\n",
      "0.8098591549295775\n",
      "KNN Score:\n",
      "0.823943661971831\n",
      "SVM Score:\n",
      "0.8661971830985915\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "# test on o.g data:\n",
    "print(\"Titanic:\")\n",
    "print(\"________________________________________\")\n",
    "print(\"L.G Score:\")\n",
    "print(LogReg.score(X_test, y_test))\n",
    "print(\"KNN Score:\")\n",
    "print(gs_knn.score(X_test, y_test))\n",
    "print(\"SVM Score:\")\n",
    "print(gs_svc.score(X_test, y_test))\n",
    "print(\"________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Final Choice\n",
    "After all of our optimization attemps we've concluded that our best Model is the SVM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
